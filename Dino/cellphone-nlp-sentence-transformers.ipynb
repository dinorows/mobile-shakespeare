{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StBKzu5s18Gz"
   },
   "source": [
    "<div style=\"text-align: right\">Dino Konstantopoulos, 3 June 2021</div>\n",
    "\n",
    "# Introducing sentence transformers\n",
    "A python package called **sentence-transformers** that has specifically been optimized for doing semantic textual similarity searches. The model creates a 1024-dimensional embedding for each sentence, and the similarity between two such sentences can then be calculated by the cosine similarity between the corresponding two vectors.\n",
    "\n",
    "A cosine similarity of 1 means the questions are identical (the angle is 0), and a cosine similarity of -1 means the questions are very different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARC Classification dataset\n",
    "\n",
    "The [ARC question classification dataset](https://allenai.org/data/arc-classification) is a dataset of 1700 questions. that went offline last week. But I found it on Amazon.\n",
    "\n",
    "We can use it as our testing ground to experiment with the affinity of our sentence embeddings.\n",
    "\n",
    "**Approach 1**: The transformer model outputs a 1024-dimensional vector for each token in our sentence. Then, we can mean-pool the vectors to generate a single sentence-level vector.\n",
    "\n",
    "**Approach 2**: We can also calculate the cosine distance between each token in our query and each token in the sentence-to-compare-with, and then mean-pool the cosine angles. Calculating the cosine similarity between all token embeddingslets us see the contributions of each token towards the final similarity score and explaining what the model is doing. \n",
    "\n",
    ">**Research Question**: Should we take the mean of all token embeddings ***prior*** to calculating cosine similarity between different sentence embeddings? Or should we see how each token embedding from the query is aligned against token embeddings in potentially matching questions? What is the best approach for our **belief models**?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries required"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "iI4103Fr15UO"
   },
   "source": [
    "!pip install transformers hdbscan pacmap\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "Let's pretend that the first question in our dataset is our original query, and try to find the closest matching entry from the rest of the questions, and contrast our approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfnAuxXD1_xe"
   },
   "source": [
    "# Download ARC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K49PF5o1t0C"
   },
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/ai2-website/data/ARC-V1-Feb2018.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Wwyr_gS1WoP"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('ARC-V1-Feb2018.zip', \"r\") as zip_obj:\n",
    "    zip_obj.extractall(\"data\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kQhnUE32F_1"
   },
   "source": [
    "# Import dataset into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTc0xdMj1WoP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./data/ARC-V1-Feb2018-2/ARC-Easy/ARC-Easy-Train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO-9Jxw-2Pcl"
   },
   "source": [
    "# Load transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHt-CzrF1WoQ"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from itertools import zip_longest\n",
    "import torch\n",
    "\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"\"\"Taken from: https://docs.python.org/3/library/itertools.html#itertools-recipes\"\"\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Mean pooling to get sentence embeddings. See:\n",
    "    https://huggingface.co/sentence-transformers/paraphrase-distilroberta-base-v1\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) # Sum columns\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "# Sentences to embed\n",
    "df = df[df.question.str.contains('\\?')]\n",
    "df.question = [s.split('?')[0] + '?' for s in df.question]\n",
    "\n",
    "# Fetch the model & tokenizer from transformers library\n",
    "model_name = 'sentence-transformers/stsb-roberta-large'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD7AFXMo2Vu8"
   },
   "source": [
    "# Create sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtymqswy1WoR",
    "outputId": "368541eb-ab18-4e68-8326-39610a40ce14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [09:57,  2.68s/it]\n"
     ]
    }
   ],
   "source": [
    "sentence_embeddings = []\n",
    "token_embeddings = []\n",
    "\n",
    "# Embed 8 sentences at a time\n",
    "for sentences in tqdm(grouper(df.question.tolist(), 8, None)):\n",
    "    \n",
    "    # Ignore sentences with None\n",
    "    valid_sentences = [s for s in sentences if s]\n",
    "\n",
    "    # Tokenize input\n",
    "    encoded_input = tokenizer(valid_sentences, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")    \n",
    "\n",
    "    # Create word embeddings\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "    # For each sentence, store a list of token embeddings; i.e. a 1024-dimensional vector for each token\n",
    "    for i, sentence in enumerate(valid_sentences):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][i])\n",
    "        embeddings = model_output[0][i]\n",
    "        token_embeddings.append(\n",
    "            [{\"token\": token, \"embedding\": embedding.detach().numpy()} for token, embedding in zip(tokens, embeddings)]\n",
    "        )    \n",
    "\n",
    "    # Pool to get sentence embeddings; i.e. generate one 1024 vector for the entire sentence\n",
    "    sentence_embeddings.append(\n",
    "        mean_pooling(model_output, encoded_input['attention_mask']).detach().numpy()\n",
    "    )\n",
    "    \n",
    "# Concatenate all of the embeddings into one numpy array of shape (n_sentences, 1024)\n",
    "sentence_embeddings = np.concatenate(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHucz4kYU-iZ"
   },
   "source": [
    "# Perform Search & Show Search Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "kO6wudej1WoS",
    "outputId": "8a4520e5-c003-473c-a413-de5734683631"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Noralize the data\n",
    "norm_data = normalize(sentence_embeddings, norm='l2')\n",
    "\n",
    "# Set QUERY & BEST MATCH IDs\n",
    "QUERY_ID = 0\n",
    "scores = np.dot(norm_data, norm_data[QUERY_ID].T)\n",
    "MATCH_ID = np.argsort(scores)[-2]\n",
    "\n",
    "\n",
    "def get_token_embeddings(embeddings_word):\n",
    "    \"\"\"Returns a list of tokens and list of embeddings\"\"\"\n",
    "    tokens, embeddings = [], []\n",
    "    for word in embeddings_word:\n",
    "        if word['token'] not in ['<s>', '<pad>', '</pad>', '</s>']:\n",
    "            tokens.append(word['token'].replace('Ä ', ''))\n",
    "            embeddings.append(word['embedding'])    \n",
    "    return tokens, normalize(embeddings, norm='l2')\n",
    "\n",
    "# Get tokens & token embeddings\n",
    "query_tokens, query_token_embeddings = get_token_embeddings(token_embeddings[QUERY_ID])\n",
    "match_tokens, match_token_embeddings = get_token_embeddings(token_embeddings[MATCH_ID])\n",
    "\n",
    "# Calculate cosine similarity between all tokens in query and match sentences\n",
    "attention = (query_token_embeddings @ match_token_embeddings.T)\n",
    "\n",
    "def plot_attention(src, trg, attention):\n",
    "    \"\"\"Plot 2D plot of cosine similarities\"\"\"\n",
    "    fig = plt.figure(dpi=150)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attention, interpolation='nearest')\n",
    "    clb = fig.colorbar(cax)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.set_xticklabels([''] + src, rotation=90)\n",
    "    ax.set_yticklabels([''] + trg) \n",
    "    \n",
    "\n",
    "plot_attention(match_tokens, query_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WHv5Jlb9oRv",
    "outputId": "1921c8f3-8aa3-4fcb-91f7-91fdcd775c26"
   },
   "outputs": [],
   "source": [
    "lengths_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_7v93JWrM_-",
    "outputId": "e502d201-146f-45ff-c0e9-da35b77b023e"
   },
   "outputs": [],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run \n",
    "Since I have trouble loading conda environments on my Jupyter notebook, I run the code in a python file on the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To think about\n",
    "Our first experiments should be to see which of the two approaches outlined herein produce best results with the ARC dataset.\n",
    "\n",
    "Also for next week, think about how can we combine LDA with transformer sentence embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `SentenceTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'A package that maps sentences into embeddings.', \n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading `stsb-roberta-large`\n",
    "Which I found [here](https://huggingface.co/models).\n",
    "\n",
    "This take 5 hours to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "model = SentenceTransformer('stsb-roberta-large')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STS_interpretation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
